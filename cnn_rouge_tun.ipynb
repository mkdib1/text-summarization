{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ed4797",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cf535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dibmir\\AppData\\Local\\anaconda3\\envs\\tmenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from util import lda_summarizer, pager_summarizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from functools import partial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bbc1c",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96994832",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # .env loading\n",
    "\n",
    "# Data path\n",
    "cnn_path = os.getenv(\"CNN_DIR\")\n",
    "bbc_path = os.getenv(\"BBC_DIR\")\n",
    "\n",
    "# lm import through 'spacy'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# stopwords definition with custom list addition\n",
    "stop_words = set(stopwords.words('english'))\n",
    "added_stopwords = {\n",
    "        \"however\", \"yet\", \"although\", \"though\", \"even though\", \"nevertheless\", \"nonetheless\",\n",
    "        \"still\", \"despite\", \"in spite of\", \"whereas\", \"alternatively\", \"instead\", \"regardless\",\n",
    "        \"notwithstanding\", \"albeit\", \"conversely\", \"be that as it may\", \"even so\", \"that said\",\n",
    "        \"even if\", \"except\", \"rather\", \"apart from\", \"despite that\", \"then again\", \"in contrast\",\n",
    "        \"after all\"\n",
    "    }\n",
    "    \n",
    "all_stopwords = set(stop_words).union(added_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c671c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda_summary(nlp, row, num_s, nt, stop_words, bonus_weight=0.6, penalty_weight=0.4):\n",
    "    try:\n",
    "        return lda_summarizer(\n",
    "            nlp=nlp,\n",
    "            # text=row['news'], # if run 'cnn_dailymail' please uncomment this line\n",
    "            text=row['content'], # if run 'bbc-news-data' please uncomment this line\n",
    "            num_sentences=num_s,\n",
    "            num_topics=nt,\n",
    "            remove_stopwords=True,\n",
    "            stop_words=all_stopwords,\n",
    "            bonus_weight=bonus_weight,\n",
    "            penalty_weight=penalty_weight\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def build_pager_summary(nlp, row, num_s, pg, lmbd_p, stop_words, model, use_mmr, bonus_weight=0.6, penalty_weight=0.4):\n",
    "    try:\n",
    "        return pager_summarizer(\n",
    "            nlp=nlp,\n",
    "            # text=row['news'], # if run 'cnn_dailymail' please uncomment this line\n",
    "            text=row['content'], # if run 'bbc-news-data' please uncomment this line\n",
    "            num_sentences=num_s,\n",
    "            use_mmr=use_mmr,\n",
    "            lambda_param=lmbd_p,\n",
    "            remove_stopwords=True,\n",
    "            stop_words=all_stopwords,\n",
    "            bonus_weight=bonus_weight,\n",
    "            penalty_weight=penalty_weight,\n",
    "            pagerank_top_k=pg,\n",
    "            embedding_model=model\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6035cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_multiple_columns(df, prediction_cols, reference_col='ref_summary'):\n",
    "    \"\"\"\n",
    "    ROUGE evaluation function using HuggingFace's evaluate library.\n",
    "\n",
    "    This function evaluates the quality of multiple predicted summary columns against a reference summary column\n",
    "    ('ref_summary') using ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-L). It returns a DataFrame reporting the average\n",
    "    f-measure scores for each summarization method (identified by the column name).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the predictions and reference summaries.\n",
    "        prediction_cols (list): List of column names containing the generated summaries to evaluate.\n",
    "        reference_col (str): Name of the column containing the reference summaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the average ROUGE scores per prediction column.\n",
    "    \"\"\"\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = {}\n",
    "\n",
    "    for pred_col in prediction_cols:\n",
    "        predictions = df[pred_col].fillna(\"\").tolist()\n",
    "        references = df[reference_col].fillna(\"\").tolist()\n",
    "\n",
    "        try:\n",
    "            scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "        except Exception:\n",
    "            scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "\n",
    "        results[pred_col] = {\n",
    "            'rouge1_f': round(scores['rouge1'], 3),\n",
    "            'rouge2_f': round(scores['rouge2'], 3),\n",
    "            'rougeL_f': round(scores['rougeL'], 3),\n",
    "        }\n",
    "\n",
    "    df_result = pd.DataFrame(results).T.reset_index()\n",
    "    df_result = df_result.rename(columns={\"index\": \"model\"})\n",
    "    return df_result.sort_values('rouge1_f', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2b998",
   "metadata": {},
   "source": [
    "# Tuning & Summary building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f5230",
   "metadata": {},
   "source": [
    "The aim of the current tuning procedure is to evaluate summarization models using the ROUGE score as the primary evaluation metric.<br>\n",
    "Since the original `bbc-news-data` dataset does not include reference summaries, the models previously developed in `model_building.py` are now tested on a different dataset: CNN/DailyMail.<br> This dataset collects news articles from both CNN and the Daily Mail, and includes the full text of each article with highlights summaries which have been written by humans.<br> These highlights consist of one or multiple sentences that are either directly extracted from the article or closely paraphrased.<br>\n",
    "Despite row data can be found here https://github.com/google-deepmind/rc-data , a preprocessed-short version of the dataset has been downloaded from https://www.kaggle.com/datasets/yatharthgautam123789/cnn-dailymail-3-0-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce8ce1",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = pd.read_csv(cnn_path, sep=\",\", usecols=['article', 'highlights']) # cnn df read\n",
    "df_cnn = df_cnn.rename(columns={'article': 'news', 'highlights': 'ref_summary'}) # col rename\n",
    "df_cnn = df_cnn.sample(frac=0.05, random_state=123) # Using 5% of the rows for sustainable tuningg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b7293",
   "metadata": {},
   "source": [
    "## Summary extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ee05b",
   "metadata": {},
   "source": [
    "A grid search approach is performed to tune hyperparameters, by taking into account the same values which have been used in `model_building.py`.<br>\n",
    "That is because of the possibility of making comparison between **BLANC** and **ROUGE** scores, although evaluated over different datasets.<br>\n",
    "To ensure the tuning process remains computationally manageable, a small random batch of the data (5%) was extracted.<br>\n",
    "The procedure returns a pandas.DataFrame containing, for each iteration of hyperparameter values, the summary column named as the specific values-combination. The values of the column are the related summaries built with that combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c35d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:15<00:00,  6.67it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.75it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.88it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.47it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.65it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.28it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.03it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.61it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.84it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.99it/s]\n",
      "100%|██████████| 500/500 [01:32<00:00,  5.39it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.64it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.46it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.57it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.63it/s]\n",
      "100%|██████████| 500/500 [01:13<00:00,  6.84it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.70it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.44it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.26it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.88it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.63it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.63it/s]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.61it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.71it/s]\n",
      "100%|██████████| 500/500 [01:32<00:00,  5.43it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.62it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.25it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.61it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.65it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.66it/s]\n",
      "100%|██████████| 500/500 [01:14<00:00,  6.71it/s]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.55it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.58it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.45it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.65it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.33it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.72it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.58it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.64it/s]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.59it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.72it/s]\n",
      "100%|██████████| 500/500 [01:32<00:00,  5.43it/s]\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.71it/s]\n",
      "100%|██████████| 500/500 [00:41<00:00, 12.17it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.51it/s]\n",
      "100%|██████████| 500/500 [01:30<00:00,  5.55it/s]\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.72it/s]\n",
      "100%|██████████| 500/500 [01:13<00:00,  6.77it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.70it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.67it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.48it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.24it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.82it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.66it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.52it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.93it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.48it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.62it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.37it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.86it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.58it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [01:14<00:00,  6.71it/s]\n",
      "100%|██████████| 500/500 [00:52<00:00,  9.55it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.58it/s]\n",
      "100%|██████████| 500/500 [01:41<00:00,  4.93it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.68it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.67it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.66it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.44it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.66it/s]\n",
      "100%|██████████| 500/500 [00:52<00:00,  9.52it/s]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.45it/s]\n",
      "100%|██████████| 500/500 [01:40<00:00,  4.96it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.62it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.67it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.67it/s]\n",
      "100%|██████████| 500/500 [01:30<00:00,  5.51it/s]\n",
      "100%|██████████| 500/500 [01:29<00:00,  5.61it/s]\n",
      "100%|██████████| 500/500 [01:14<00:00,  6.75it/s]\n",
      "100%|██████████| 500/500 [00:50<00:00,  9.82it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.79it/s]\n",
      "100%|██████████| 500/500 [01:41<00:00,  4.94it/s]\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.70it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.76it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77it/s]\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.44it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.67it/s]\n",
      "100%|██████████| 500/500 [00:52<00:00,  9.58it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.74it/s]\n",
      "100%|██████████| 500/500 [01:41<00:00,  4.93it/s]\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.62it/s]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.69it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.71it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.45it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:28<00:00,  5.62it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:14<00:00,  6.74it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'lda_summary_ns{num}_bw{b_w}_pw{p_w}'] = df_cnn.progress_apply(lda_func, axis=1)\n",
      "100%|██████████| 500/500 [00:51<00:00,  9.64it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.62it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:44<00:00,  4.80it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:30<00:00,  5.51it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.97it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.48it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.71it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:51<00:00,  9.69it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.86it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:40<00:00,  4.95it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.73it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.75it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.86it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.49it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.69it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:13<00:00,  6.82it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'lda_summary_ns{num}_bw{b_w}_pw{p_w}'] = df_cnn.progress_apply(lda_func, axis=1)\n",
      "100%|██████████| 500/500 [00:51<00:00,  9.74it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.94it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:40<00:00,  4.98it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.69it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.89it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.97it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:30<00:00,  5.50it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.70it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:51<00:00,  9.72it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.73it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:40<00:00,  4.97it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.73it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.83it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.83it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:31<00:00,  5.49it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n",
      "100%|██████████| 500/500 [01:27<00:00,  5.69it/s]\n",
      "C:\\Users\\dibmir\\AppData\\Local\\Temp\\ipykernel_3040\\2053712982.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Grid search using custom-defined parameter values\n",
    "b_ws = [0.0, 0.6]\n",
    "p_ws = [0.0, 0.6]\n",
    "lambda_param = [0.2, 0.8]\n",
    "num_sent = [2, 5]\n",
    "num_top = [2, 4]\n",
    "pgs = [None, 10]\n",
    "models = [None, model]\n",
    "use_mmr = [True, False]\n",
    "\n",
    "for num in num_sent:\n",
    "    for n_t in num_top:\n",
    "        for b_w in b_ws:\n",
    "            for p_w in p_ws:\n",
    "                # LDA\n",
    "                lda_func = partial(build_lda_summary, nlp, num_s=num, nt=n_t, stop_words=all_stopwords, bonus_weight=b_w, penalty_weight=p_w)\n",
    "                df_cnn[f'lda_summary_ns{num}_nt{n_t}_bw{b_w}_pw{p_w}'] = df_cnn.progress_apply(lda_func, axis=1)\n",
    "\n",
    "                # Pagerank-MMR\n",
    "                for lmbd in lambda_param:\n",
    "                    for pg in pgs:\n",
    "                        for mod in models:\n",
    "                            for um in use_mmr:\n",
    "                                pager_func = partial(build_pager_summary, nlp, pg=pg, num_s=num, lmbd_p=lmbd, stop_words=all_stopwords, bonus_weight=b_w, penalty_weight=p_w, model=mod, use_mmr=um)\n",
    "                                df_cnn[f'pager_summary_ns{num}_bw{b_w}_pw{p_w}_l{lmbd}_pg{pg}_mod{mod}_um{um}'] = df_cnn.progress_apply(pager_func, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb877094",
   "metadata": {},
   "source": [
    "## ROUGE computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240cd56e",
   "metadata": {},
   "source": [
    "Once the dataframe containing the summary columns is arrenged, ROUGE evaluation is performed column-wise: this means that the ROUGE scores are computed for each summary in a given column and then averaged to obtain an overall score for that summarization method.<br>\n",
    "The evaluation returns three distinct ROUGE metrics:\n",
    "\n",
    "* **rouge_1**: measures the overlap between the generated and reference summaries based on unigrams.<br>\n",
    "* **rouge_2**: evaluates the overlap at the level of bigrams.<br>\n",
    "* **rouge_L**: the metric is based on the Longest Common Subsequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rouge1_f",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rouge2_f",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rougeL_f",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "28d93291-7c77-40c0-a9d7-d5d393cf5252",
       "rows": [
        [
         "77",
         "pager_summary_ns5_bw0.0_pw0.0_l0.8_pgNone_modNone_umTrue",
         "0.24371800000000016",
         "0.05207399999999995",
         "0.1456659999999998"
        ],
        [
         "94",
         "pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modNone_umTrue",
         "0.24169000000000024",
         "0.05021799999999994",
         "0.14245000000000005"
        ],
        [
         "96",
         "pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.24133000000000013",
         "0.044979999999999985",
         "0.13991000000000012"
        ],
        [
         "69",
         "pager_summary_ns5_bw0.0_pw0.0_l0.2_pgNone_modNone_umTrue",
         "0.24102200000000004",
         "0.043663999999999946",
         "0.14079200000000008"
        ],
        [
         "86",
         "pager_summary_ns5_bw0.0_pw0.6_l0.2_pgNone_modNone_umTrue",
         "0.2408679999999999",
         "0.04178199999999992",
         "0.13823600000000005"
        ],
        [
         "120",
         "pager_summary_ns5_bw0.6_pw0.6_l0.2_pgNone_modNone_umTrue",
         "0.24085200000000004",
         "0.04211999999999995",
         "0.1392220000000001"
        ],
        [
         "83",
         "pager_summary_ns5_bw0.0_pw0.0_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.2401099999999999",
         "0.048277999999999946",
         "0.14020399999999994"
        ],
        [
         "56",
         "pager_summary_ns2_bw0.6_pw0.6_l0.2_pg10_modNone_umTrue",
         "0.24005999999999997",
         "0.04393799999999996",
         "0.14831"
        ],
        [
         "79",
         "pager_summary_ns5_bw0.0_pw0.0_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23981199999999991",
         "0.047188",
         "0.14046800000000004"
        ],
        [
         "103",
         "pager_summary_ns5_bw0.6_pw0.0_l0.2_pgNone_modNone_umTrue",
         "0.2396600000000003",
         "0.04433799999999996",
         "0.13839000000000018"
        ],
        [
         "124",
         "pager_summary_ns5_bw0.6_pw0.6_l0.2_pg10_modNone_umTrue",
         "0.23959599999999992",
         "0.04856999999999994",
         "0.13895999999999997"
        ],
        [
         "73",
         "pager_summary_ns5_bw0.0_pw0.0_l0.2_pg10_modNone_umTrue",
         "0.23840199999999998",
         "0.04885000000000001",
         "0.13974200000000012"
        ],
        [
         "90",
         "pager_summary_ns5_bw0.0_pw0.6_l0.2_pg10_modNone_umTrue",
         "0.23794200000000013",
         "0.048354",
         "0.137642"
        ],
        [
         "107",
         "pager_summary_ns5_bw0.6_pw0.0_l0.2_pg10_modNone_umTrue",
         "0.23743999999999987",
         "0.04849399999999994",
         "0.13896599999999995"
        ],
        [
         "43",
         "pager_summary_ns2_bw0.6_pw0.0_l0.8_pgNone_modNone_umTrue",
         "0.23715399999999986",
         "0.044485999999999956",
         "0.1484200000000001"
        ],
        [
         "47",
         "pager_summary_ns2_bw0.6_pw0.0_l0.8_pg10_modNone_umTrue",
         "0.23711799999999977",
         "0.04386399999999999",
         "0.1471659999999999"
        ],
        [
         "128",
         "pager_summary_ns5_bw0.6_pw0.6_l0.8_pgNone_modNone_umTrue",
         "0.23691400000000004",
         "0.05080999999999998",
         "0.14111200000000002"
        ],
        [
         "64",
         "pager_summary_ns2_bw0.6_pw0.6_l0.8_pg10_modNone_umTrue",
         "0.2366559999999998",
         "0.042491999999999974",
         "0.14768999999999996"
        ],
        [
         "75",
         "pager_summary_ns5_bw0.0_pw0.0_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23587800000000003",
         "0.04570199999999998",
         "0.1379160000000001"
        ],
        [
         "130",
         "pager_summary_ns5_bw0.6_pw0.6_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23564199999999996",
         "0.046666000000000006",
         "0.13695399999999996"
        ],
        [
         "100",
         "pager_summary_ns5_bw0.0_pw0.6_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23560000000000011",
         "0.047463999999999985",
         "0.13843400000000003"
        ],
        [
         "66",
         "pager_summary_ns2_bw0.6_pw0.6_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23545200000000008",
         "0.04064399999999997",
         "0.14609200000000017"
        ],
        [
         "111",
         "pager_summary_ns5_bw0.6_pw0.0_l0.8_pgNone_modNone_umTrue",
         "0.23529200000000008",
         "0.05046199999999999",
         "0.13954399999999995"
        ],
        [
         "49",
         "pager_summary_ns2_bw0.6_pw0.0_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23528599999999997",
         "0.04206599999999999",
         "0.14701000000000014"
        ],
        [
         "39",
         "pager_summary_ns2_bw0.6_pw0.0_l0.2_pg10_modNone_umTrue",
         "0.23519800000000007",
         "0.04362200000000001",
         "0.14831999999999998"
        ],
        [
         "98",
         "pager_summary_ns5_bw0.0_pw0.6_l0.8_pg10_modNone_umTrue",
         "0.23451000000000002",
         "0.05054799999999997",
         "0.13805799999999996"
        ],
        [
         "134",
         "pager_summary_ns5_bw0.6_pw0.6_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.233784",
         "0.04941400000000001",
         "0.13729400000000003"
        ],
        [
         "5",
         "pager_summary_ns2_bw0.0_pw0.0_l0.2_pg10_modNone_umTrue",
         "0.23369800000000018",
         "0.042092000000000004",
         "0.14509400000000006"
        ],
        [
         "117",
         "pager_summary_ns5_bw0.6_pw0.0_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23332600000000006",
         "0.04835799999999993",
         "0.13615800000000017"
        ],
        [
         "22",
         "pager_summary_ns2_bw0.0_pw0.6_l0.2_pg10_modNone_umTrue",
         "0.23324400000000028",
         "0.04197399999999996",
         "0.143986"
        ],
        [
         "113",
         "pager_summary_ns5_bw0.6_pw0.0_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.2332020000000001",
         "0.04529799999999998",
         "0.13530800000000004"
        ],
        [
         "126",
         "pager_summary_ns5_bw0.6_pw0.6_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23274000000000009",
         "0.04441399999999997",
         "0.13542000000000007"
        ],
        [
         "132",
         "pager_summary_ns5_bw0.6_pw0.6_l0.8_pg10_modNone_umTrue",
         "0.23231600000000002",
         "0.051652",
         "0.13910400000000003"
        ],
        [
         "109",
         "pager_summary_ns5_bw0.6_pw0.0_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23213000000000011",
         "0.04539799999999995",
         "0.13734000000000007"
        ],
        [
         "81",
         "pager_summary_ns5_bw0.0_pw0.0_l0.8_pg10_modNone_umTrue",
         "0.2316799999999999",
         "0.04863599999999992",
         "0.13694399999999998"
        ],
        [
         "30",
         "pager_summary_ns2_bw0.0_pw0.6_l0.8_pg10_modNone_umTrue",
         "0.23167399999999974",
         "0.03937599999999999",
         "0.14411799999999994"
        ],
        [
         "60",
         "pager_summary_ns2_bw0.6_pw0.6_l0.8_pgNone_modNone_umTrue",
         "0.231418",
         "0.041142",
         "0.14527800000000007"
        ],
        [
         "45",
         "pager_summary_ns2_bw0.6_pw0.0_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.231372",
         "0.039824",
         "0.14465999999999998"
        ],
        [
         "15",
         "pager_summary_ns2_bw0.0_pw0.0_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.231322",
         "0.04175399999999997",
         "0.1482759999999999"
        ],
        [
         "62",
         "pager_summary_ns2_bw0.6_pw0.6_l0.8_pgNone_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.2311720000000001",
         "0.04087199999999995",
         "0.1446640000000001"
        ],
        [
         "92",
         "pager_summary_ns5_bw0.0_pw0.6_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23102000000000011",
         "0.045935999999999984",
         "0.13544399999999993"
        ],
        [
         "32",
         "pager_summary_ns2_bw0.0_pw0.6_l0.8_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.23095400000000002",
         "0.04087400000000001",
         "0.14679199999999998"
        ],
        [
         "13",
         "pager_summary_ns2_bw0.0_pw0.0_l0.8_pg10_modNone_umTrue",
         "0.23042399999999996",
         "0.041257999999999954",
         "0.1452479999999999"
        ],
        [
         "9",
         "pager_summary_ns2_bw0.0_pw0.0_l0.8_pgNone_modNone_umTrue",
         "0.2299439999999999",
         "0.04221999999999997",
         "0.1460960000000001"
        ],
        [
         "115",
         "pager_summary_ns5_bw0.6_pw0.0_l0.8_pg10_modNone_umTrue",
         "0.22919400000000006",
         "0.04921799999999999",
         "0.13565200000000005"
        ],
        [
         "58",
         "pager_summary_ns2_bw0.6_pw0.6_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.22810800000000012",
         "0.036324000000000016",
         "0.14424799999999988"
        ],
        [
         "51",
         "lda_summary_ns2_bw0.6_pw0.6",
         "0.22794199999999998",
         "0.03874799999999999",
         "0.140552"
        ],
        [
         "34",
         "lda_summary_ns2_bw0.6_pw0.0",
         "0.22730799999999984",
         "0.03877399999999999",
         "0.13646000000000008"
        ],
        [
         "7",
         "pager_summary_ns2_bw0.0_pw0.0_l0.2_pg10_modSentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)_umTrue",
         "0.2272080000000001",
         "0.03986399999999997",
         "0.14448400000000006"
        ],
        [
         "26",
         "pager_summary_ns2_bw0.0_pw0.6_l0.8_pgNone_modNone_umTrue",
         "0.22666400000000006",
         "0.039051999999999996",
         "0.14477399999999999"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 136
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1_f</th>\n",
       "      <th>rouge2_f</th>\n",
       "      <th>rougeL_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.0_l0.8_pgNone_modN...</td>\n",
       "      <td>0.243718</td>\n",
       "      <td>0.052074</td>\n",
       "      <td>0.145666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modN...</td>\n",
       "      <td>0.241690</td>\n",
       "      <td>0.050218</td>\n",
       "      <td>0.142450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modS...</td>\n",
       "      <td>0.241330</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.139910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.0_l0.2_pgNone_modN...</td>\n",
       "      <td>0.241022</td>\n",
       "      <td>0.043664</td>\n",
       "      <td>0.140792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.6_l0.2_pgNone_modN...</td>\n",
       "      <td>0.240868</td>\n",
       "      <td>0.041782</td>\n",
       "      <td>0.138236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.6_l0.2_pg10_modSen...</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.106366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>pager_summary_ns5_bw0.6_pw0.0_l0.2_pg10_modSen...</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.106366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modS...</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.106366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>pager_summary_ns5_bw0.6_pw0.0_l0.2_pgNone_modS...</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.106366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>pager_summary_ns5_bw0.6_pw0.6_l0.8_pg10_modSen...</td>\n",
       "      <td>0.177420</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.106366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 model  rouge1_f  rouge2_f  \\\n",
       "77   pager_summary_ns5_bw0.0_pw0.0_l0.8_pgNone_modN...  0.243718  0.052074   \n",
       "94   pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modN...  0.241690  0.050218   \n",
       "96   pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modS...  0.241330  0.044980   \n",
       "69   pager_summary_ns5_bw0.0_pw0.0_l0.2_pgNone_modN...  0.241022  0.043664   \n",
       "86   pager_summary_ns5_bw0.0_pw0.6_l0.2_pgNone_modN...  0.240868  0.041782   \n",
       "..                                                 ...       ...       ...   \n",
       "93   pager_summary_ns5_bw0.0_pw0.6_l0.2_pg10_modSen...  0.177420  0.041240   \n",
       "110  pager_summary_ns5_bw0.6_pw0.0_l0.2_pg10_modSen...  0.177420  0.041240   \n",
       "97   pager_summary_ns5_bw0.0_pw0.6_l0.8_pgNone_modS...  0.177420  0.041240   \n",
       "106  pager_summary_ns5_bw0.6_pw0.0_l0.2_pgNone_modS...  0.177420  0.041240   \n",
       "135  pager_summary_ns5_bw0.6_pw0.6_l0.8_pg10_modSen...  0.177420  0.041240   \n",
       "\n",
       "     rougeL_f  \n",
       "77   0.145666  \n",
       "94   0.142450  \n",
       "96   0.139910  \n",
       "69   0.140792  \n",
       "86   0.138236  \n",
       "..        ...  \n",
       "93   0.106366  \n",
       "110  0.106366  \n",
       "97   0.106366  \n",
       "106  0.106366  \n",
       "135  0.106366  \n",
       "\n",
       "[136 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_cols = [col for col in df_cnn.columns if col.startswith('lda_summary') or col.startswith('pager_summary')]\n",
    "df_rouge = compute_rouge_multiple_columns(df_cnn, prediction_cols)\n",
    "# df_rouge.to_csv('Data/results/rouge_scores.csv', sep=\";\", index=False) # results df writing\n",
    "df_rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebff635",
   "metadata": {},
   "source": [
    "## BBC Dataframe with 'summary' columns building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77614d",
   "metadata": {},
   "source": [
    "After ROUGE performing, the best rouge1 scored models is used to extract summaries of `bbc-news-data` fulltext.<br>\n",
    "In order to get 3 main different types of model-built summaries, the following are taken into account:\n",
    "\n",
    "* pagerank + MMR using Tf-Idf representation\n",
    "* pagerank + MMR using a sentence transformers-based model for sentence embedding\n",
    "* lda \n",
    "\n",
    "The rest of hyperparameter values are set before each model running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c44b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbc = pd.read_csv(bbc_path, sep=\"\\t\") # bbc df read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [02:19<00:00, 15.98it/s]\n",
      "100%|██████████| 2225/2225 [04:26<00:00,  8.36it/s]\n",
      "100%|██████████| 2225/2225 [03:09<00:00, 11.76it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "ns_1 = 5\n",
    "lamb_1 = 0.8\n",
    "bw_1 = 0.0\n",
    "pw_1 = 0.0\n",
    "um_1 = True\n",
    "pg_1= None\n",
    "mod_1 = None\n",
    "\n",
    "pager_func_1 = partial(build_pager_summary, nlp, num_s=ns_1, pg=pg_1, \n",
    "                     lmbd_p=lamb_1, stop_words=all_stopwords, model=mod_1, \n",
    "                     use_mmr=um_1, bonus_weight = bw_1, penalty_weight = pw_1)\n",
    "df_bbc['summary_tfidf'] = df_bbc.progress_apply(pager_func_1, axis=1)\n",
    "\n",
    "ns_2 = 5\n",
    "lamb_2 = 0.8\n",
    "bw_2 = 0.0\n",
    "pw_2 = 0.6\n",
    "um_2 = True\n",
    "pg_2= None\n",
    "mod_2 = model\n",
    "\n",
    "pager_func_2 = partial(build_pager_summary, nlp, num_s=ns_2, pg=pg_2, \n",
    "                     lmbd_p=lamb_2, stop_words=all_stopwords, model=mod_2, \n",
    "                     use_mmr=um_2, bonus_weight = bw_2, penalty_weight = pw_2)\n",
    "df_bbc['summary_mod'] = df_bbc.progress_apply(pager_func_2, axis=1)\n",
    "\n",
    "ns_3 = 5\n",
    "nt = 2\n",
    "bw_3 = 0.0\n",
    "pw_3 = 0.6\n",
    "\n",
    "lda_func = partial(build_lda_summary, nlp, num_s=ns_3, nt=nt, \n",
    "                     stop_words=all_stopwords, bonus_weight = bw_3, \n",
    "                     penalty_weight = pw_3)\n",
    "df_bbc['summary_lda'] = df_bbc.progress_apply(lda_func, axis=1)\n",
    "\n",
    "# df_bbc.to_csv('Data/output/bbc-news-data-summaries-new.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
